{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f4a0bcf",
   "metadata": {},
   "source": [
    "# ETL da camada silver para camada gold\n",
    "\n",
    "Esta célula importa todas as bibliotecas necessárias para o processo de ETL.\n",
    "Aqui são carregados os pacotes para manipulação de dados (pyspark), conexão com o banco de dados PostgreSQL (psycopg), controle de mensagens de erro (sys) e tratamento de avisos (warnings).\n",
    "Ela deve ser executada antes de qualquer outra célula, pois fornece as dependências básicas que serão usadas nas etapas de Extract, Transform e Load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e210a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import psycopg2 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from psycopg2 import sql\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, DecimalType, TimestampType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99434b2",
   "metadata": {},
   "source": [
    "# 1. CONFIGURAÇÕES GERAIS\n",
    "\n",
    "como costume e para evitar possivei erro preferismo configurar o todo sparky logo no topo para ter tudo que precisamos pronto \n",
    "aproveitamos e nessa parte para Ignora avisos chatos de versão/depreciação e tambem arrega variáveis da .env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65006f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "warnings.filterwarnings('ignore') \n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23446190",
   "metadata": {},
   "source": [
    "### 1.1 DRIVERS E CAMINHOS\n",
    "nessa parte e onde feito o ajuste o caminho se necessário. O driver JDBC é vital para o Spark falar com o Postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8dc8205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Driver JDBC localizado: /home/emivalto/workspace/BD2V/SDBD2---INEP/Transformer/postgresql-42.7.8.jar\n"
     ]
    }
   ],
   "source": [
    "\n",
    "JAR_NAME = \"postgresql-42.7.8.jar\"\n",
    "JAR_PATH = os.path.abspath(JAR_NAME)\n",
    "\n",
    "if not os.path.exists(JAR_PATH):\n",
    "    print(f\"AVISO: Driver JDBC '{JAR_NAME}' não encontrado na pasta atual.\")\n",
    "    print(\"O Spark não conseguirá salvar no banco sem ele.\")\n",
    "else:\n",
    "    print(f\" Driver JDBC localizado: {JAR_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb85bdb",
   "metadata": {},
   "source": [
    "### 1.2 INICIALIZAÇÃO DA SESSÃO SPARK\n",
    " esta e uma das fasses cruciais tambem pois e onde finalizamos sessão antiga se existir para garantir que o JAR seja carregado corretamente, e onde configuramos o Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae0ce1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iniciando motor Spark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/24 01:55:05 WARN Utils: Your hostname, Emivalto, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "26/01/24 01:55:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "26/01/24 01:55:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 4.1.1 ativo e pronto para análise!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Iniciando motor Spark...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL_Enem_Gold_Layer\") \\\n",
    "    .config(\"spark.driver.memory\", \"5g\") \\\n",
    "    .config(\"spark.jars\", JAR_PATH) \\\n",
    "    .config(\"spark.driver.extraClassPath\", JAR_PATH) \\\n",
    "    .config(\"spark.executor.extraClassPath\", JAR_PATH) \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"CORRECTED\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark {spark.version} ativo e pronto para análise!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148de6d9",
   "metadata": {},
   "source": [
    "### 1.3 VARIÁVEIS DE CONEXÃO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "527eca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\",\n",
    "    \"dbname\": \"dados_inep\",\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"l1l2r1r2\" \n",
    "}\n",
    "\n",
    "JDBC_URL = f\"jdbc:postgresql://{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c6a95",
   "metadata": {},
   "source": [
    "# 1. Extract\n",
    "\n",
    "Esta célula define as configurações de conexão com o banco de dados PostgreSQL e monta a consulta SQL que será usada para extrair os dados.\n",
    "Ela cria variáveis com credenciais, monta o nome completo da tabela (schema.tabela) e gera a query SELECT * FROM silver.listings, além de preparar a connection string usada na etapa de conexão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2b0c396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Preparando leitura da tabela: silver.microdados_enem\n",
      " URL de Conexão: jdbc:postgresql://localhost:5432/dados_inep\n",
      "\n",
      " Sucesso! Dados da camada Silver carregados.\n",
      "Schema carregado do Banco:\n",
      "root\n",
      " |-- nu_inscricao: long (nullable = true)\n",
      " |-- tp_faixa_etaria: integer (nullable = true)\n",
      " |-- tp_sexo: string (nullable = true)\n",
      " |-- tp_estado_civil: integer (nullable = true)\n",
      " |-- tp_cor_raca: integer (nullable = true)\n",
      " |-- tp_nacionalidade: integer (nullable = true)\n",
      " |-- tp_st_conclusao: integer (nullable = true)\n",
      " |-- tp_ano_concluiu: integer (nullable = true)\n",
      " |-- tp_escola: integer (nullable = true)\n",
      " |-- in_treineiro: integer (nullable = true)\n",
      " |-- co_municipio_prova: integer (nullable = true)\n",
      " |-- no_municipio_prova: string (nullable = true)\n",
      " |-- co_uf_prova: integer (nullable = true)\n",
      " |-- sg_uf_prova: string (nullable = true)\n",
      " |-- tp_presenca_cn: integer (nullable = true)\n",
      " |-- tp_presenca_ch: integer (nullable = true)\n",
      " |-- tp_presenca_lc: integer (nullable = true)\n",
      " |-- tp_presenca_mt: integer (nullable = true)\n",
      " |-- co_prova_cn: integer (nullable = true)\n",
      " |-- co_prova_ch: integer (nullable = true)\n",
      " |-- co_prova_lc: integer (nullable = true)\n",
      " |-- co_prova_mt: integer (nullable = true)\n",
      " |-- nu_nota_cn: decimal(10,2) (nullable = true)\n",
      " |-- nu_nota_ch: decimal(10,2) (nullable = true)\n",
      " |-- nu_nota_lc: decimal(10,2) (nullable = true)\n",
      " |-- nu_nota_mt: decimal(10,2) (nullable = true)\n",
      " |-- tx_respostas_cn: string (nullable = true)\n",
      " |-- tx_respostas_ch: string (nullable = true)\n",
      " |-- tx_respostas_lc: string (nullable = true)\n",
      " |-- tx_respostas_mt: string (nullable = true)\n",
      " |-- tp_lingua: integer (nullable = true)\n",
      " |-- tx_gabarito_cn: string (nullable = true)\n",
      " |-- tx_gabarito_ch: string (nullable = true)\n",
      " |-- tx_gabarito_lc: string (nullable = true)\n",
      " |-- tx_gabarito_mt: string (nullable = true)\n",
      " |-- tp_status_redacao: integer (nullable = true)\n",
      " |-- nu_nota_comp1: decimal(10,2) (nullable = true)\n",
      " |-- nu_nota_comp2: decimal(10,2) (nullable = true)\n",
      " |-- nu_nota_comp3: decimal(10,2) (nullable = true)\n",
      " |-- nu_nota_comp4: decimal(10,2) (nullable = true)\n",
      " |-- nu_nota_comp5: decimal(10,2) (nullable = true)\n",
      " |-- nu_nota_redacao: decimal(10,2) (nullable = true)\n",
      " |-- q001: string (nullable = true)\n",
      " |-- q002: string (nullable = true)\n",
      " |-- q003: string (nullable = true)\n",
      " |-- q004: string (nullable = true)\n",
      " |-- q005: integer (nullable = true)\n",
      " |-- q006: string (nullable = true)\n",
      " |-- q007: string (nullable = true)\n",
      " |-- q008: string (nullable = true)\n",
      " |-- q009: string (nullable = true)\n",
      " |-- q010: string (nullable = true)\n",
      " |-- q011: string (nullable = true)\n",
      " |-- q012: string (nullable = true)\n",
      " |-- q013: string (nullable = true)\n",
      " |-- q014: string (nullable = true)\n",
      " |-- q015: string (nullable = true)\n",
      " |-- q016: string (nullable = true)\n",
      " |-- q017: string (nullable = true)\n",
      " |-- q018: string (nullable = true)\n",
      " |-- q019: string (nullable = true)\n",
      " |-- q020: string (nullable = true)\n",
      " |-- q021: string (nullable = true)\n",
      " |-- q022: string (nullable = true)\n",
      " |-- q023: string (nullable = true)\n",
      " |-- q024: string (nullable = true)\n",
      " |-- q025: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DB_SCHEMA = \"silver\"\n",
    "TABLE_NAME = \"microdados_enem\"\n",
    "FULL_TABLE_NAME = f\"{DB_SCHEMA}.{TABLE_NAME}\" \n",
    "\n",
    "\n",
    "def get_jdbc_connection_info():\n",
    "    load_dotenv()\n",
    "    \n",
    "    url = os.getenv('DB_JDBC_URL') \n",
    "    if url:\n",
    "        return url\n",
    "\n",
    "    DB_USER = \"admin\"\n",
    "    DB_PASS = \"l1l2r1r2\"\n",
    "    DB_HOST = \"localhost\"\n",
    "    DB_PORT = \"5432\"\n",
    "    DB_NAME = \"dados_inep\"\n",
    "\n",
    "    return f\"jdbc:postgresql://{DB_HOST}:{DB_PORT}/{DB_NAME}\", DB_USER, DB_PASS\n",
    "\n",
    "jdbc_url, db_user, db_pass = get_jdbc_connection_info()\n",
    "\n",
    "print(f\" Preparando leitura da tabela: {FULL_TABLE_NAME}\")\n",
    "print(f\" URL de Conexão: {jdbc_url}\")\n",
    "\n",
    "try:\n",
    "    df_silver = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", FULL_TABLE_NAME) \\\n",
    "        .option(\"user\", db_user) \\\n",
    "        .option(\"password\", db_pass) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .load()\n",
    "\n",
    "    print(\"\\n Sucesso! Dados da camada Silver carregados.\")\n",
    "    print(\"Schema carregado do Banco:\")\n",
    "    df_silver.printSchema()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Erro ao ler do banco: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c50bf",
   "metadata": {},
   "source": [
    "# 2 CARGA DOS DADOS (Leitura JDBC via Spark) \n",
    "\n",
    "Esta proxima célula ira executa a extração dos dados do banco PostgreSQL.\n",
    "\n",
    "Ela tem que estabelece a conexão usando as configurações definidas anteriormente, converte o objeto SQL em uma query, executa a consulta e carrega o resultado no DataFrame df.\n",
    "\n",
    "Em caso de falha na conexão ou na leitura, deve ser exibido uma mensagem de erro detalhada e encerra o processo.\n",
    "\n",
    "Em Big Data, constumasse usar o `spark.read` para criar um ponteiro distribuído para os dados, mais robusto.\n",
    "\n",
    "afim de melhorar e ver o se aconteceu algum erro no final e feito umas captura dos erros genéricos do Java/Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e636a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Estabelecendo conexão com o Spark JDBC...\n",
      " Conexão estabelecida.\n",
      " Tabela 'silver.microdados_enem' mapeada com sucesso para o Spark!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\" Estabelecendo conexão com o Spark JDBC...\")\n",
    "    \n",
    "    df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", FULL_TABLE_NAME) \\\n",
    "        .option(\"user\", db_user) \\\n",
    "        .option(\"password\", db_pass) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .load()\n",
    "\n",
    "    print(\" Conexão estabelecida.\")\n",
    "    print(f\" Tabela '{FULL_TABLE_NAME}' mapeada com sucesso para o Spark!\")\n",
    "    \n",
    "\n",
    "except Exception as e:\n",
    "   \n",
    "    print(f\"\\n Ocorreu um erro ao conectar ou ler o banco de dados via Spark \")\n",
    "    print(f\"Erro detalhado: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafddbdc",
   "metadata": {},
   "source": [
    "### 2.1 VALIDAÇÃO DA CARGA\n",
    "Estas células e basicamente para validar a quantidade de dados esta vindo corretamente e exibem um resumo simples do resultado da extração, mostrando o número total de registros carregados no DataFrame df, as primeiras cinco tuplas e os tipos de cada dado.\n",
    "Elas servem para confirmar visualmente que a consulta foi executada com sucesso e quantas linhas foram retornadas do banco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03b4e2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Contando registros no DataFrame...\n",
      "Total de linhas carregadas: 509954\n",
      "\n",
      " --- Estrutura da Tabela (Schema) ---\n",
      "root\n",
      " |-- nu_inscricao: long (nullable = true)\n",
      " |-- tp_faixa_etaria: integer (nullable = true)\n",
      " |-- tp_sexo: string (nullable = true)\n",
      " |-- tp_estado_civil: integer (nullable = true)\n",
      " |-- tp_cor_raca: integer (nullable = true)\n",
      " |-- tp_nacionalidade: integer (nullable = true)\n",
      " |-- tp_st_conclusao: integer (nullable = true)\n",
      " |-- tp_ano_concluiu: integer (nullable = true)\n",
      " |-- tp_escola: integer (nullable = true)\n",
      " |-- in_treineiro: integer (nullable = true)\n",
      " |-- co_municipio_prova: integer (nullable = true)\n",
      " |-- no_municipio_prova: string (nullable = true)\n",
      " |-- co_uf_prova: integer (nullable = true)\n",
      " |-- sg_uf_prova: string (nullable = true)\n",
      " |-- tp_presenca_cn: integer (nullable = true)\n",
      " |-- tp_presenca_ch: integer (nullable = true)\n",
      " |-- tp_presenca_lc: integer (nullable = true)\n",
      " |-- tp_presenca_mt: integer (nullable = true)\n",
      " |-- co_prova_cn: integer (nullable = true)\n",
      " |-- co_prova_ch: integer (nullable = true)\n",
      " |-- co_prova_lc: integer (nullable = true)\n",
      " |-- co_prova_mt: integer (nullable = true)\n",
      " |-- nu_nota_cn: decimal(10,2) (nullable = true)\n",
      " |-- nu_nota_ch: decimal(10,2) (nullable = true)\n",
      " |-- nu_nota_lc: decimal(10,2) (nullable = true)\n",
      " |-- nu_nota_mt: decimal(10,2) (nullable = true)\n",
      " |-- tx_respostas_cn: string (nullable = true)\n",
      " |-- tx_respostas_ch: string (nullable = true)\n",
      " |-- tx_respostas_lc: string (nullable = true)\n",
      " |-- tx_respostas_mt: string (nullable = true)\n",
      " |-- tp_lingua: integer (nullable = true)\n",
      " |-- tx_gabarito_cn: string (nullable = true)\n",
      " |-- tx_gabarito_ch: string (nullable = true)\n",
      " |-- tx_gabarito_lc: string (nullable = true)\n",
      " |-- tx_gabarito_mt: string (nullable = true)\n",
      " |-- tp_status_redacao: integer (nullable = true)\n",
      " |-- nu_nota_comp1: decimal(10,2) (nullable = true)\n",
      " |-- nu_nota_comp2: decimal(10,2) (nullable = true)\n",
      " |-- nu_nota_comp3: decimal(10,2) (nullable = true)\n",
      " |-- nu_nota_comp4: decimal(10,2) (nullable = true)\n",
      " |-- nu_nota_comp5: decimal(10,2) (nullable = true)\n",
      " |-- nu_nota_redacao: decimal(10,2) (nullable = true)\n",
      " |-- q001: string (nullable = true)\n",
      " |-- q002: string (nullable = true)\n",
      " |-- q003: string (nullable = true)\n",
      " |-- q004: string (nullable = true)\n",
      " |-- q005: integer (nullable = true)\n",
      " |-- q006: string (nullable = true)\n",
      " |-- q007: string (nullable = true)\n",
      " |-- q008: string (nullable = true)\n",
      " |-- q009: string (nullable = true)\n",
      " |-- q010: string (nullable = true)\n",
      " |-- q011: string (nullable = true)\n",
      " |-- q012: string (nullable = true)\n",
      " |-- q013: string (nullable = true)\n",
      " |-- q014: string (nullable = true)\n",
      " |-- q015: string (nullable = true)\n",
      " |-- q016: string (nullable = true)\n",
      " |-- q017: string (nullable = true)\n",
      " |-- q018: string (nullable = true)\n",
      " |-- q019: string (nullable = true)\n",
      " |-- q020: string (nullable = true)\n",
      " |-- q021: string (nullable = true)\n",
      " |-- q022: string (nullable = true)\n",
      " |-- q023: string (nullable = true)\n",
      " |-- q024: string (nullable = true)\n",
      " |-- q025: string (nullable = true)\n",
      "\n",
      "\n",
      "--- Visualização dos Dados (Amostra) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/24 01:55:27 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+-------+---------------+-----------+----------------+---------------+---------------+---------+------------+------------------+------------------+-----------+-----------+--------------+--------------+--------------+--------------+-----------+-----------+-----------+-----------+----------+----------+----------+----------+---------------------------------------------+---------------------------------------------+--------------------------------------------------+---------------------------------------------+---------+---------------------------------------------+---------------------------------------------+--------------------------------------------------+---------------------------------------------+-----------------+-------------+-------------+-------------+-------------+-------------+---------------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|nu_inscricao|tp_faixa_etaria|tp_sexo|tp_estado_civil|tp_cor_raca|tp_nacionalidade|tp_st_conclusao|tp_ano_concluiu|tp_escola|in_treineiro|co_municipio_prova|no_municipio_prova|co_uf_prova|sg_uf_prova|tp_presenca_cn|tp_presenca_ch|tp_presenca_lc|tp_presenca_mt|co_prova_cn|co_prova_ch|co_prova_lc|co_prova_mt|nu_nota_cn|nu_nota_ch|nu_nota_lc|nu_nota_mt|tx_respostas_cn                              |tx_respostas_ch                              |tx_respostas_lc                                   |tx_respostas_mt                              |tp_lingua|tx_gabarito_cn                               |tx_gabarito_ch                               |tx_gabarito_lc                                    |tx_gabarito_mt                               |tp_status_redacao|nu_nota_comp1|nu_nota_comp2|nu_nota_comp3|nu_nota_comp4|nu_nota_comp5|nu_nota_redacao|q001|q002|q003|q004|q005|q006|q007|q008|q009|q010|q011|q012|q013|q014|q015|q016|q017|q018|q019|q020|q021|q022|q023|q024|q025|\n",
      "+------------+---------------+-------+---------------+-----------+----------------+---------------+---------------+---------+------------+------------------+------------------+-----------+-----------+--------------+--------------+--------------+--------------+-----------+-----------+-----------+-----------+----------+----------+----------+----------+---------------------------------------------+---------------------------------------------+--------------------------------------------------+---------------------------------------------+---------+---------------------------------------------+---------------------------------------------+--------------------------------------------------+---------------------------------------------+-----------------+-------------+-------------+-------------+-------------+-------------+---------------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|210052231727|6              |M      |1              |1          |1               |1              |4              |1        |0           |3550308           |São Paulo         |35         |SP         |1             |1             |1             |1             |912        |882        |891        |901        |677.80    |691.00    |661.00    |792.40    |AADCBAEBEEDBBABEDDCECBCCDABCAEACBDCCCDBDBEBCB|EAAAEADBEAEBBABEECBBADEBBBEDEBADBCEDBEBBCAEEE|ACABB99999BCEADCBCACCEEAADDAEBEDDDBBDAEADCDCCEBBED|DACAEBACDDCBBECCCBCBECADDEEDCAECADCBBBABDBDCE|0        |DABCEDEBEEBBCABEDDCBCBECDADCDAACBDCCCDBBBEBAB|EBAAEADBCACBBABEECBAAEEBBBADCBADBCEDDEBBCAEAB|ACABBACAEADCEADABDACDCEABDDADBEDDDEBBACCDDDCCEBBBB|DCCAEBABDDCABEECCBCCEXADDCEECDEBADCABBDBDEDCE|1                |160.00       |160.00       |200.00       |180.00       |160.00       |860.00         |F   |E   |D   |F   |3   |G   |A   |C   |C   |B   |A   |B   |B   |B   |A   |B   |A   |A   |C   |A   |B   |D   |B   |C   |B   |\n",
      "|210053931371|2              |F      |0              |1          |1               |2              |0              |3        |0           |3548500           |Santos            |35         |SP         |1             |1             |1             |1             |909        |881        |892        |899        |710.30    |684.70    |658.00    |813.10    |EDECABEEADCBBBACCDCEBEEBDCBBEABCBCBDCDCEBADCD|BEECCAACBADBCADEAAEDEBEBBBBDBCBAADEBBBADCABBA|ABEAC99999CEBDDACEBCDCBDCBDBADDBBDEDEEEAADCDAEACDE|EBAEBABDDCECCADCDCDCEEAADDABBDBDEDDDDCCADDCAB|0        |EDECABEDABCBEBACCDCBBEEBBCBDAAECDCBDCDBBBADCD|BEECCAEABADBCAEBAAEEDDEBBBADBCBAAEEBBBADCCBBA|ABBACAAECACDBDDADEBDDCBDCEDBEDDBBBBDCEEAADABACACDC|EEXEBABDDCECCBCCECDCAEBADEABBDBDEDCEDCCADDCAB|1                |160.00       |180.00       |140.00       |160.00       |160.00       |800.00         |F   |F   |D   |D   |4   |N   |A   |C   |D   |B   |A   |B   |B   |B   |B   |B   |A   |B   |B   |A   |A   |E   |A   |D   |B   |\n",
      "|210051482416|3              |M      |1              |1          |1               |2              |0              |2        |0           |3541000           |Praia Grande      |35         |SP         |1             |1             |1             |1             |909        |881        |892        |899        |661.70    |579.70    |521.20    |654.70    |EBEEABEBABCDEBCECDCBBEECBDBDEACDDCEBCDAECABBD|BCEACDEABC*CEDEAECEDCBABBCADBBAAABECBCABCBBDA|AACAC99999DDBDEAACDDCEBCCCBDDEBCCDEDEBEAACCAEEACCE|EBAECABDCCCCDBECBCCCBBADDEADBADEBBACBCDADDBBC|0        |EDECABEDABCBEBACCDCBBEEBBCBDAAECDCBDCDBBBADCD|BEECCAEABADBCAEBAAEEDDEBBBADBCBAAEEBBBADCCBBA|ABBACAAECACDBDDADEBDDCBDCEDBEDDBBBBDCEEAADABACACDC|EEXEBABDDCECCBCCECDCAEBADEABBDBDEDCEDCCADDCAB|1                |80.00        |120.00       |80.00        |80.00        |40.00        |400.00         |E   |C   |E   |B   |3   |B   |A   |B   |C   |A   |A   |A   |B   |B   |B   |B   |A   |B   |B   |A   |A   |C   |B   |B   |B   |\n",
      "|210052247190|2              |F      |1              |4          |4               |2              |0              |2        |0           |3550308           |São Paulo         |35         |SP         |0             |1             |1             |0             |0          |879        |889        |0          |0.00      |595.10    |589.30    |0.00      |Nao Respondido                               |BACCEAADAEDBEBBACDBABDECCAAACDCABAEABDBCACEDC|CAABB99999BDDDEADCDDDAADCACBEDCEAABACCBBBAAACCCBDB|Nao Respondido                               |0        |Nao Respondido                               |BADCBADBCEDDEBBADBCABEECCAEABEBAAECBBABAAEEBB|CAABBCAEAABDCDEADCEDBEDDDABBBBDEBCEADABDBACACDCDDC|Nao Respondido                               |1                |160.00       |200.00       |180.00       |180.00       |140.00       |860.00         |D   |E   |B   |C   |3   |E   |A   |D   |D   |C   |A   |B   |B   |B   |A   |B   |A   |A   |C   |A   |B   |C   |B   |C   |B   |\n",
      "|210052806093|3              |M      |1              |3          |1               |2              |0              |2        |0           |3510609           |Carapicuíba       |35         |SP         |1             |1             |1             |1             |910        |879        |889        |900        |564.50    |573.30    |527.50    |675.40    |ACDDCECBBECACBDCAEDDCBDEAADACCCEECCEDBAEBDABE|EDDCEBDDBDDBEBAACCEDBBEDCAEABDADDCEBDADBACECB|EADBB99999ADCEEADEEEDEADCACBEACEDEDBCEBDDABAECBCBA|DDCDDDCEEBADEDBADEAEEAADCEACABDECDDAECEADCAEE|0        |ADCDCDBBBECDCCDCBCBDABCAADCBEBABEEBBBCABEDEDE|BADCBADBCEDDEBBADBCABEECCAEABEBAAECBBABAAEEBB|CAABBCAEAABDCDEADCEDBEDDDABBBBDEBCEADABDBACACDCDDC|DDCABDCCAEDCEBBDBDAEEBADCAECDBCCCCDDCEEBABXEE|1                |120.00       |120.00       |120.00       |120.00       |80.00        |560.00         |E   |E   |D   |B   |5   |D   |A   |B   |C   |B   |B   |B   |B   |B   |A   |B   |A   |B   |D   |A   |B   |E   |A   |B   |B   |\n",
      "+------------+---------------+-------+---------------+-----------+----------------+---------------+---------------+---------+------------+------------------+------------------+-----------+-----------+--------------+--------------+--------------+--------------+-----------+-----------+-----------+-----------+----------+----------+----------+----------+---------------------------------------------+---------------------------------------------+--------------------------------------------------+---------------------------------------------+---------+---------------------------------------------+---------------------------------------------+--------------------------------------------------+---------------------------------------------+-----------------+-------------+-------------+-------------+-------------+-------------+---------------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\" Contando registros no DataFrame...\")\n",
    "qtd_linhas = df.count()\n",
    "print(f\"Total de linhas carregadas: {qtd_linhas}\")\n",
    "\n",
    "print(\"\\n --- Estrutura da Tabela (Schema) ---\")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\n--- Visualização dos Dados (Amostra) ---\")\n",
    "\n",
    "df.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f14dd3",
   "metadata": {},
   "source": [
    "# 2. Transform\n",
    "\n",
    "### RENOMEAÇÃO E REMOÇÃO DE COLUNAS \n",
    "\n",
    " 1. Definição do Mapa de Colunas (De -> Para)\n",
    " Adapte as chaves abaixo para as colunas do ENEM (ex: 'nu_nota_mt': 'nota_matematica')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d2cbebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iniciando processo de transformação dos dados...\n",
      " Renomeando colunas...\n",
      " Colunas renomeadas e desnecessárias removidas.\n",
      "Novas colunas: ['NUM_INS', 'COD_FAI_ETA', 'TIP_SEX', 'COD_EST_CIV', 'COD_COR_RAC', 'COD_NAC', 'COD_SIT_CON', 'NUM_ANO_CON', 'IND_TRE', 'COD_MUN', 'NOM_MUN', 'IND_PRE_NAT', 'IND_PRE_HUM', 'IND_PRE_LIN', 'IND_PRE_MAT', 'COD_PRV_NAT', 'COD_PRV_HUM', 'COD_PRV_LIN', 'COD_PRV_MAT', 'VAL_NOT_NAT', 'VAL_NOT_HUM', 'VAL_NOT_LIN', 'VAL_NOT_MAT', 'TIP_LIN', 'COD_SIT_RED', 'VAL_NOT_RED', 'COD_ESC_PAI', 'COD_ESC_MAE', 'COD_OCU_PAI', 'COD_OCU_MAE', 'QTD_PES_RES', 'COD_REN_FAM', 'COD_EMP_DOM', 'COD_POS_BAN', 'COD_POS_QUA', 'COD_POS_CAR', 'COD_POS_MOT', 'COD_POS_GEL', 'COD_POS_FRE', 'COD_POS_LAV', 'COD_POS_SEC', 'COD_POS_MIC', 'COD_POS_LOU', 'COD_POS_ASP', 'COD_POS_TEL', 'COD_POS_DVD', 'COD_POS_TVA', 'COD_POS_CEL', 'COD_POS_FIX', 'COD_POS_COM', 'IND_ACE_INT']\n",
      "\n",
      "--- Visualização dos Dados (Amostra) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-------+-----------+-----------+-------+-----------+-----------+-------+-------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|NUM_INS     |COD_FAI_ETA|TIP_SEX|COD_EST_CIV|COD_COR_RAC|COD_NAC|COD_SIT_CON|NUM_ANO_CON|IND_TRE|COD_MUN|NOM_MUN     |IND_PRE_NAT|IND_PRE_HUM|IND_PRE_LIN|IND_PRE_MAT|COD_PRV_NAT|COD_PRV_HUM|COD_PRV_LIN|COD_PRV_MAT|VAL_NOT_NAT|VAL_NOT_HUM|VAL_NOT_LIN|VAL_NOT_MAT|TIP_LIN|COD_SIT_RED|VAL_NOT_RED|COD_ESC_PAI|COD_ESC_MAE|COD_OCU_PAI|COD_OCU_MAE|QTD_PES_RES|COD_REN_FAM|COD_EMP_DOM|COD_POS_BAN|COD_POS_QUA|COD_POS_CAR|COD_POS_MOT|COD_POS_GEL|COD_POS_FRE|COD_POS_LAV|COD_POS_SEC|COD_POS_MIC|COD_POS_LOU|COD_POS_ASP|COD_POS_TEL|COD_POS_DVD|COD_POS_TVA|COD_POS_CEL|COD_POS_FIX|COD_POS_COM|IND_ACE_INT|\n",
      "+------------+-----------+-------+-----------+-----------+-------+-----------+-----------+-------+-------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|210052231727|6          |M      |1          |1          |1      |1          |4          |0      |3550308|São Paulo   |1          |1          |1          |1          |912        |882        |891        |901        |677.80     |691.00     |661.00     |792.40     |0      |1          |860.00     |F          |E          |D          |F          |3          |G          |A          |C          |C          |B          |A          |B          |B          |B          |A          |B          |A          |A          |C          |A          |B          |D          |B          |C          |B          |\n",
      "|210053931371|2          |F      |0          |1          |1      |2          |0          |0      |3548500|Santos      |1          |1          |1          |1          |909        |881        |892        |899        |710.30     |684.70     |658.00     |813.10     |0      |1          |800.00     |F          |F          |D          |D          |4          |N          |A          |C          |D          |B          |A          |B          |B          |B          |B          |B          |A          |B          |B          |A          |A          |E          |A          |D          |B          |\n",
      "|210051482416|3          |M      |1          |1          |1      |2          |0          |0      |3541000|Praia Grande|1          |1          |1          |1          |909        |881        |892        |899        |661.70     |579.70     |521.20     |654.70     |0      |1          |400.00     |E          |C          |E          |B          |3          |B          |A          |B          |C          |A          |A          |A          |B          |B          |B          |B          |A          |B          |B          |A          |A          |C          |B          |B          |B          |\n",
      "|210052247190|2          |F      |1          |4          |4      |2          |0          |0      |3550308|São Paulo   |0          |1          |1          |0          |0          |879        |889        |0          |0.00       |595.10     |589.30     |0.00       |0      |1          |860.00     |D          |E          |B          |C          |3          |E          |A          |D          |D          |C          |A          |B          |B          |B          |A          |B          |A          |A          |C          |A          |B          |C          |B          |C          |B          |\n",
      "|210052806093|3          |M      |1          |3          |1      |2          |0          |0      |3510609|Carapicuíba |1          |1          |1          |1          |910        |879        |889        |900        |564.50     |573.30     |527.50     |675.40     |0      |1          |560.00     |E          |E          |D          |B          |5          |D          |A          |B          |C          |B          |B          |B          |B          |B          |A          |B          |A          |B          |D          |A          |B          |E          |A          |B          |B          |\n",
      "+------------+-----------+-------+-----------+-----------+-------+-----------+-----------+-------+-------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"\\n Iniciando processo de transformação dos dados...\")\n",
    "\n",
    "mapa_colunas = {\n",
    "    'nu_inscricao': 'NUM_INS',\n",
    "    'tp_faixa_etaria': 'COD_FAI_ETA',\n",
    "    'tp_sexo': 'TIP_SEX',\n",
    "    'tp_estado_civil': 'COD_EST_CIV',\n",
    "    'tp_cor_raca': 'COD_COR_RAC',\n",
    "    'tp_nacionalidade': 'COD_NAC',\n",
    "    'tp_st_conclusao': 'COD_SIT_CON',\n",
    "    'tp_ano_concluiu': 'NUM_ANO_CON',\n",
    "    'in_treineiro': 'IND_TRE',\n",
    "    'co_municipio_prova': 'COD_MUN',\n",
    "    'no_municipio_prova': 'NOM_MUN',\n",
    "    'tp_presenca_cn': 'IND_PRE_NAT',\n",
    "    'tp_presenca_ch': 'IND_PRE_HUM',\n",
    "    'tp_presenca_lc': 'IND_PRE_LIN',\n",
    "    'tp_presenca_mt': 'IND_PRE_MAT',\n",
    "    'co_prova_cn': 'COD_PRV_NAT',\n",
    "    'co_prova_ch': 'COD_PRV_HUM',\n",
    "    'co_prova_lc': 'COD_PRV_LIN',\n",
    "    'co_prova_mt': 'COD_PRV_MAT',\n",
    "    'nu_nota_cn': 'VAL_NOT_NAT',\n",
    "    'nu_nota_ch': 'VAL_NOT_HUM',\n",
    "    'nu_nota_lc': 'VAL_NOT_LIN',\n",
    "    'nu_nota_mt': 'VAL_NOT_MAT',\n",
    "    'tp_lingua': 'TIP_LIN',\n",
    "    'tp_status_redacao': 'COD_SIT_RED',\n",
    "    'nu_nota_redacao': 'VAL_NOT_RED',\n",
    "    'q001': 'COD_ESC_PAI',\n",
    "    'q002': 'COD_ESC_MAE',\n",
    "    'q003': 'COD_OCU_PAI',\n",
    "    'q004': 'COD_OCU_MAE',\n",
    "    'q005': 'QTD_PES_RES',\n",
    "    'q006': 'COD_REN_FAM',\n",
    "    'q007': 'COD_EMP_DOM',\n",
    "    'q008': 'COD_POS_BAN',\n",
    "    'q009': 'COD_POS_QUA',\n",
    "    'q010': 'COD_POS_CAR',\n",
    "    'q011': 'COD_POS_MOT',\n",
    "    'q012': 'COD_POS_GEL',\n",
    "    'q013': 'COD_POS_FRE',\n",
    "    'q014': 'COD_POS_LAV',\n",
    "    'q015': 'COD_POS_SEC',\n",
    "    'q016': 'COD_POS_MIC',\n",
    "    'q017': 'COD_POS_LOU',\n",
    "    'q018': 'COD_POS_ASP',\n",
    "    'q019': 'COD_POS_TEL',\n",
    "    'q020': 'COD_POS_DVD',\n",
    "    'q021': 'COD_POS_TVA',\n",
    "    'q022': 'COD_POS_CEL',\n",
    "    'q023': 'COD_POS_FIX',\n",
    "    'q024': 'COD_POS_COM',\n",
    "    'q025': 'IND_ACE_INT'\n",
    "}\n",
    "\n",
    "\n",
    "print(\" Renomeando colunas...\")\n",
    "for coluna_antiga, coluna_nova in mapa_colunas.items():\n",
    "    df = df.withColumnRenamed(coluna_antiga, coluna_nova)\n",
    "\n",
    "\n",
    "cols_to_drop = ['tp_escola', 'co_uf_prova', 'sg_uf_prova','tx_gabarito_cn', 'tx_gabarito_ch', 'tx_gabarito_lc', 'tx_gabarito_mt',\n",
    "                'tx_respostas_cn', 'tx_respostas_ch', 'tx_respostas_lc', 'tx_respostas_mt', 'nu_nota_comp1', 'nu_nota_comp2', \n",
    "                'nu_nota_comp3', 'nu_nota_comp4', 'nu_nota_comp5']\n",
    "\n",
    "df = df.drop(*cols_to_drop)\n",
    "\n",
    "print(\" Colunas renomeadas e desnecessárias removidas.\")\n",
    "print(\"Novas colunas:\", df.columns)\n",
    "\n",
    "print(\"\\n--- Visualização dos Dados (Amostra) ---\")\n",
    "\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35c38a",
   "metadata": {},
   "source": [
    "### Alocação das colunas \n",
    "\n",
    "##### ORGANIZAÇÃO E CONVERSÃO DE TIPOS\n",
    "nessa parte armazenamos as colunas nas variaveis certar para tabela dimenção e tabela fato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4285f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Colunas segmentadas para Dimensões:\n",
      "\n",
      " Parâmetros: ['NUM_INS', 'COD_FAI_ETA', 'TIP_SEX', 'COD_EST_CIV', 'COD_COR_RAC', 'COD_NAC', 'COD_SIT_CON', 'NUM_ANO_CON', 'IND_TRE']\n",
      "\n",
      " Localização: ['COD_MUN', 'NOM_MUN']\n",
      "\n",
      " Socioeconômica: ['COD_ESC_PAI', 'COD_ESC_MAE', 'COD_OCU_PAI', 'COD_OCU_MAE', 'QTD_PES_RES', 'COD_REN_FAM', 'COD_EMP_DOM', 'COD_POS_BAN', 'COD_POS_QUA', 'COD_POS_CAR', 'COD_POS_MOT', 'COD_POS_GEL', 'COD_POS_FRE', 'COD_POS_LAV', 'COD_POS_SEC', 'COD_POS_MIC', 'COD_POS_LOU', 'COD_POS_ASP', 'COD_POS_TEL', 'COD_POS_DVD', 'COD_POS_TVA', 'COD_POS_CEL', 'COD_POS_FIX', 'COD_POS_COM', 'IND_ACE_INT']\n",
      "\n",
      " Prova: ['TIP_LIN', 'COD_PRV_NAT', 'COD_PRV_HUM', 'COD_PRV_LIN', 'COD_PRV_MAT', 'COD_SIT_RED']\n",
      "\n",
      " Fato: ['NUM_INS', 'VAL_NOT_NAT', 'VAL_NOT_HUM', 'VAL_NOT_LIN', 'VAL_NOT_MAT', 'VAL_NOT_RED', 'IND_PRE_NAT', 'IND_PRE_HUM', 'IND_PRE_LIN', 'IND_PRE_MAT']\n",
      "\n",
      "Fato: Todas as outras colunas restantes. ['NUM_INS', 'COD_FAI_ETA', 'TIP_SEX', 'COD_EST_CIV', 'COD_COR_RAC', 'COD_NAC', 'COD_SIT_CON', 'NUM_ANO_CON', 'IND_TRE', 'COD_MUN', 'NOM_MUN', 'IND_PRE_NAT', 'IND_PRE_HUM', 'IND_PRE_LIN', 'IND_PRE_MAT', 'COD_PRV_NAT', 'COD_PRV_HUM', 'COD_PRV_LIN', 'COD_PRV_MAT', 'VAL_NOT_NAT', 'VAL_NOT_HUM', 'VAL_NOT_LIN', 'VAL_NOT_MAT', 'TIP_LIN', 'COD_SIT_RED', 'VAL_NOT_RED', 'COD_ESC_PAI', 'COD_ESC_MAE', 'COD_OCU_PAI', 'COD_OCU_MAE', 'QTD_PES_RES', 'COD_REN_FAM', 'COD_EMP_DOM', 'COD_POS_BAN', 'COD_POS_QUA', 'COD_POS_CAR', 'COD_POS_MOT', 'COD_POS_GEL', 'COD_POS_FRE', 'COD_POS_LAV', 'COD_POS_SEC', 'COD_POS_MIC', 'COD_POS_LOU', 'COD_POS_ASP', 'COD_POS_TEL', 'COD_POS_DVD', 'COD_POS_TVA', 'COD_POS_CEL', 'COD_POS_FIX', 'COD_POS_COM', 'IND_ACE_INT']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_par = [ 'NUM_INS' , 'COD_FAI_ETA', 'TIP_SEX', 'COD_EST_CIV', 'COD_COR_RAC', 'COD_NAC', 'COD_SIT_CON', 'NUM_ANO_CON', 'IND_TRE']\n",
    "\n",
    "df_loc = [ 'COD_MUN', 'NOM_MUN' ]\n",
    "\n",
    "df_soc = ['COD_ESC_PAI', 'COD_ESC_MAE', 'COD_OCU_PAI', 'COD_OCU_MAE', 'QTD_PES_RES', 'COD_REN_FAM', 'COD_EMP_DOM', 'COD_POS_BAN', 'COD_POS_QUA',\n",
    "          'COD_POS_CAR', 'COD_POS_MOT', 'COD_POS_GEL', 'COD_POS_FRE', 'COD_POS_LAV', 'COD_POS_SEC', 'COD_POS_MIC', 'COD_POS_LOU', 'COD_POS_ASP',\n",
    "          'COD_POS_TEL', 'COD_POS_DVD', 'COD_POS_TVA', 'COD_POS_CEL', 'COD_POS_FIX', 'COD_POS_COM', 'IND_ACE_INT' ]\n",
    "\n",
    "df_prv = ['TIP_LIN', 'COD_PRV_NAT', 'COD_PRV_HUM', 'COD_PRV_LIN', 'COD_PRV_MAT', 'COD_SIT_RED']\n",
    "\n",
    "df_fat = ['NUM_INS','VAL_NOT_NAT', 'VAL_NOT_HUM', 'VAL_NOT_LIN', 'VAL_NOT_MAT', 'VAL_NOT_RED', 'IND_PRE_NAT', 'IND_PRE_HUM', 'IND_PRE_LIN', \n",
    "          'IND_PRE_MAT']\n",
    "\n",
    "print(\" Colunas segmentadas para Dimensões:\")\n",
    "print(\"\\n Parâmetros:\", df_par)\n",
    "print(\"\\n Localização:\", df_loc)\n",
    "print(\"\\n Socioeconômica:\", df_soc)\n",
    "print(\"\\n Prova:\", df_prv)\n",
    "print(\"\\n Fato:\", df_fat)\n",
    "\n",
    "\n",
    "print(\"\\nFato: Todas as outras colunas restantes.\",df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27bb3c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Calculando volumes das Dimensões e Fato (isso pode levar alguns segundos)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " REGISTROS PREPARADOS PARA CARGA (GOLD)\n",
      " Dimensão Participante (dim_par): 509,954 registros únicos\n",
      " Dimensão Localização  (dim_loc): 210 municípios únicos\n",
      " Dimensão Socioecon.   (dim_soc): 492,641 perfis únicos\n",
      " Dimensão Prova        (dim_prv): 495 variações de prova\n",
      " Tabela Fato (fato_enem):         509,954 registros totais\n",
      "--- Estrutura da Tabela Fato ---\n",
      "root\n",
      " |-- NUM_INS: long (nullable = true)\n",
      " |-- VAL_NOT_NAT: decimal(10,2) (nullable = true)\n",
      " |-- VAL_NOT_HUM: decimal(10,2) (nullable = true)\n",
      " |-- VAL_NOT_LIN: decimal(10,2) (nullable = true)\n",
      " |-- VAL_NOT_MAT: decimal(10,2) (nullable = true)\n",
      " |-- VAL_NOT_RED: decimal(10,2) (nullable = true)\n",
      " |-- IND_PRE_NAT: integer (nullable = true)\n",
      " |-- IND_PRE_HUM: integer (nullable = true)\n",
      " |-- IND_PRE_LIN: integer (nullable = true)\n",
      " |-- IND_PRE_MAT: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\" Calculando volumes das Dimensões e Fato (isso pode levar alguns segundos)...\")\n",
    "\n",
    "\n",
    "dim_participante = df.select(*df_par).distinct()\n",
    "dim_localizacao  = df.select(*df_loc).distinct()\n",
    "dim_socioecon    = df.select(*df_soc).distinct()\n",
    "dim_prova        = df.select(*df_prv).distinct()\n",
    "\n",
    "fato_enem        = df.select(*df_fat) \n",
    "\n",
    "qtd_par = dim_participante.count()\n",
    "qtd_loc = dim_localizacao.count()\n",
    "qtd_soc = dim_socioecon.count()\n",
    "qtd_prv = dim_prova.count()\n",
    "qtd_fat = fato_enem.count()\n",
    "\n",
    "\n",
    "\n",
    "print(\" REGISTROS PREPARADOS PARA CARGA (GOLD)\")\n",
    "\n",
    "print(f\" Dimensão Participante (dim_par): {qtd_par:,} registros únicos\")\n",
    "print(f\" Dimensão Localização  (dim_loc): {qtd_loc:,} municípios únicos\")\n",
    "print(f\" Dimensão Socioecon.   (dim_soc): {qtd_soc:,} perfis únicos\")\n",
    "print(f\" Dimensão Prova        (dim_prv): {qtd_prv:,} variações de prova\")\n",
    "print(f\" Tabela Fato (fato_enem):         {qtd_fat:,} registros totais\")\n",
    "\n",
    "print(\"--- Estrutura da Tabela Fato ---\")\n",
    "fato_enem.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a4968",
   "metadata": {},
   "source": [
    "# 3. Load\n",
    "\n",
    "Esta célula configura os parâmetros de conexão com o banco de dados do Data Warehouse (gold) e valida se todas as variáveis geradas na etapa de transformação estão disponíveis na memória.\n",
    "Ela garante que o ambiente esteja pronto antes de iniciar a fase de carga, evitando erros por falta de dados ou variáveis necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a69243a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Verificando pré-requisitos para carga no schema 'gold'...\n",
      "Tudo certo! Todas as variáveis (DataFrames e Conexão) estão carregadas.\n"
     ]
    }
   ],
   "source": [
    "DB_SCHEMA_GOLD = \"gold\"\n",
    "\n",
    "vars_to_check = [\n",
    "    'dim_participante', # tabela DIM_PAR\n",
    "    'dim_localizacao',  # tabela DIM_LOC\n",
    "    'dim_socioecon',    # tabela DIM_SOC\n",
    "    'dim_prova',        # tabela DIM_PRV\n",
    "    'fato_enem',        # tabela FAT_DES\n",
    "    'jdbc_url'   \n",
    "]\n",
    "\n",
    "print(f\" Verificando pré-requisitos para carga no schema '{DB_SCHEMA_GOLD}'...\")\n",
    "\n",
    "for v in vars_to_check:\n",
    "    if v not in globals():\n",
    "        \n",
    "        raise RuntimeError(f\" Variável ausente: {v}. Por favor, rode a célula anterior de preparação das dimensões.\")\n",
    "\n",
    "print(\"Tudo certo! Todas as variáveis (DataFrames e Conexão) estão carregadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbc4405",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65bbf22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lendo script DDL em: ../gold/ddl.sql\n",
      " Arquivo lido com sucesso.\n",
      "\n",
      " Aplicando estrutura (tabelas e chaves) no PostgreSQL...\n",
      " Sucesso! Schema 'gold' e tabelas criadas/recriadas.\n"
     ]
    }
   ],
   "source": [
    "CAMINHO_DDL_GOLD = \"../gold/ddl.sql\"\n",
    "\n",
    "print(f\" Lendo script DDL em: {CAMINHO_DDL_GOLD}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(CAMINHO_DDL_GOLD, 'r', encoding='utf-8') as f:\n",
    "        ddl_gold_script = f.read()\n",
    "    print(\" Arquivo lido com sucesso.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\" Erro Crítico: O arquivo '{CAMINHO_DDL_GOLD}' não foi encontrado.\")\n",
    "    print(\"Certifique-se de salvar o conteúdo do DDL corrigido no arquivo.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n Aplicando estrutura (tabelas e chaves) no PostgreSQL...\")\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",      \n",
    "        port=\"5432\",           \n",
    "        database=\"dados_inep\", \n",
    "        user=\"admin\",          \n",
    "        password=\"l1l2r1r2\" \n",
    "    )\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        \n",
    "        cur.execute(ddl_gold_script)\n",
    "        conn.commit() \n",
    "        \n",
    "    conn.close()\n",
    "    print(\" Sucesso! Schema 'gold' e tabelas criadas/recriadas.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Erro ao executar o DDL no banco: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901aa404",
   "metadata": {},
   "source": [
    "# CARGA DA DIMENSÃO PARTICIPANTE (DIM_PAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "572addd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Preparando Dimensão Participante (DIM_PAR)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/24 01:58:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/24 01:58:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/24 01:58:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Inserindo 509954 registros na tabela gold.DIM_PAR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/24 01:58:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/24 01:58:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/24 01:58:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/24 01:58:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/24 01:58:49 ERROR Executor: Exception in task 0.0 in stage 55.0 (TID 95)\n",
      "java.sql.BatchUpdateException: Batch entry 0 INSERT INTO gold.DIM_PAR (\"par_srk\",\"num_ins\",\"cod_fai_eta\",\"tip_sex\",\"cod_est_civ\",\"cod_cor_rac\",\"cod_nac\",\"cod_sit_con\",\"num_ano_con\",\"ind_tre\") VALUES (('1'::int4),('210051014332'::int8),('3'::int4),('F'),('1'::int4),('3'::int4),('1'::int4),('2'::int4),('0'::int4),('0'::int4)) was aborted: ERROR: duplicate key value violates unique constraint \"dim_par_pkey\"\n",
      "  Detail: Key (par_srk)=(1) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2422)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2154)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1502)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1527)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:566)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:891)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:915)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1778)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"dim_par_pkey\"\n",
      "  Detail: Key (par_srk)=(1) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2736)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\t... 24 more\n",
      "26/01/24 01:58:49 WARN TaskSetManager: Lost task 0.0 in stage 55.0 (TID 95) (10.255.255.254 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO gold.DIM_PAR (\"par_srk\",\"num_ins\",\"cod_fai_eta\",\"tip_sex\",\"cod_est_civ\",\"cod_cor_rac\",\"cod_nac\",\"cod_sit_con\",\"num_ano_con\",\"ind_tre\") VALUES (('1'::int4),('210051014332'::int8),('3'::int4),('F'),('1'::int4),('3'::int4),('1'::int4),('2'::int4),('0'::int4),('0'::int4)) was aborted: ERROR: duplicate key value violates unique constraint \"dim_par_pkey\"\n",
      "  Detail: Key (par_srk)=(1) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2422)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2154)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1502)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1527)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:566)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:891)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:915)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1778)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"dim_par_pkey\"\n",
      "  Detail: Key (par_srk)=(1) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2736)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\t... 24 more\n",
      "\n",
      "26/01/24 01:58:49 ERROR TaskSetManager: Task 0 in stage 55.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Erro na carga Spark: An error occurred while calling o269.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 55.0 failed 1 times, most recent failure: Lost task 0.0 in stage 55.0 (TID 95) (10.255.255.254 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO gold.DIM_PAR (\"par_srk\",\"num_ins\",\"cod_fai_eta\",\"tip_sex\",\"cod_est_civ\",\"cod_cor_rac\",\"cod_nac\",\"cod_sit_con\",\"num_ano_con\",\"ind_tre\") VALUES (('1'::int4),('210051014332'::int8),('3'::int4),('F'),('1'::int4),('3'::int4),('1'::int4),('2'::int4),('0'::int4),('0'::int4)) was aborted: ERROR: duplicate key value violates unique constraint \"dim_par_pkey\"\n",
      "  Detail: Key (par_srk)=(1) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2422)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2154)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1502)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1527)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:566)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:891)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:915)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1778)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"dim_par_pkey\"\n",
      "  Detail: Key (par_srk)=(1) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2736)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\t... 24 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:323)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1017)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2496)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2517)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2561)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n",
      "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1045)\n",
      "\tat org.apache.spark.sql.classic.Dataset.$anonfun$foreachPartition$1(Dataset.scala:1496)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.classic.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:2252)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:177)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:308)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:250)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withNewRDDExecutionId(Dataset.scala:2250)\n",
      "\tat org.apache.spark.sql.classic.Dataset.foreachPartition(Dataset.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:80)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:177)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:308)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:250)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:194)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:467)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:194)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:155)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:239)\n",
      "\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:592)\n",
      "\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
      "\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\n",
      "\t\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\n",
      "\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\n",
      "\t\tat scala.collection.immutable.List.foreach(List.scala:323)\n",
      "\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\n",
      "\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\n",
      "\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\n",
      "\t\tat scala.Option.foreach(Option.scala:437)\n",
      "\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\n",
      "\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\n",
      "\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\n",
      "\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\n",
      "\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
      "\t\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1017)\n",
      "\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2496)\n",
      "\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2517)\n",
      "\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2536)\n",
      "\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2561)\n",
      "\t\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1047)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\t\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n",
      "\t\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1045)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.$anonfun$foreachPartition$1(Dataset.scala:1496)\n",
      "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:2252)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:177)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:285)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:139)\n",
      "\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:139)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:308)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:138)\n",
      "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:92)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:250)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.withNewRDDExecutionId(Dataset.scala:2250)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.foreachPartition(Dataset.scala:1496)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:1017)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:80)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\n",
      "\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\n",
      "\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\n",
      "\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:185)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:177)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:285)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:139)\n",
      "\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:139)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:308)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:138)\n",
      "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:92)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:250)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:185)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:184)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:194)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:491)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:491)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:467)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:194)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:155)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\t... 17 more\n",
      "Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO gold.DIM_PAR (\"par_srk\",\"num_ins\",\"cod_fai_eta\",\"tip_sex\",\"cod_est_civ\",\"cod_cor_rac\",\"cod_nac\",\"cod_sit_con\",\"num_ano_con\",\"ind_tre\") VALUES (('1'::int4),('210051014332'::int8),('3'::int4),('F'),('1'::int4),('3'::int4),('1'::int4),('2'::int4),('0'::int4),('0'::int4)) was aborted: ERROR: duplicate key value violates unique constraint \"dim_par_pkey\"\n",
      "  Detail: Key (par_srk)=(1) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2422)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2154)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1502)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1527)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:566)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:891)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:915)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1778)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\t... 1 more\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"dim_par_pkey\"\n",
      "  Detail: Key (par_srk)=(1) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2736)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\t... 24 more\n",
      "\n",
      " Garantindo registro 'Não Informado' (ID -1)...\n",
      " Erro ao inserir registro Unknown: column \"PAR_SRK\" of relation \"dim_par\" does not exist\n",
      "LINE 2:         INSERT INTO gold.DIM_PAR (\"PAR_SRK\", \"NUM_INS\", \"COD...\n",
      "                                          ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import psycopg2 \n",
    "\n",
    "print(\" Preparando Dimensão Participante (DIM_PAR)...\")\n",
    "\n",
    "w = Window.orderBy(\"NUM_INS\")\n",
    "\n",
    "df_dim_par_final = df.select(*df_par).distinct() \\\n",
    "    .withColumn(\"PAR_SRK\", F.row_number().over(w))\n",
    "\n",
    "cols_ordem = [\"PAR_SRK\"] + df_par\n",
    "df_dim_par_final = df_dim_par_final.select(*cols_ordem)\n",
    "\n",
    "print(f\" Inserindo {df_dim_par_final.count()} registros na tabela gold.DIM_PAR...\")\n",
    "\n",
    "try:\n",
    "    df_dim_par_final.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", \"gold.DIM_PAR\") \\\n",
    "        .option(\"user\", db_user) \\\n",
    "        .option(\"password\", db_pass) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "    \n",
    "    print(\" Carga dos participantes concluída.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Erro na carga Spark: {e}\")\n",
    "print(\" Garantindo registro 'Não Informado' (ID -1)...\")\n",
    "\n",
    "try:\n",
    "\n",
    "    colunas_sql = \", \".join([f'\"{col}\"' for col in df_par])\n",
    "    \n",
    "    placeholders = \", \".join([\"%s\"] * len(df_par)) \n",
    "    valores_unknown = [-1] + [None] * len(df_par)\n",
    "    sql_unknown = f\"\"\"\n",
    "        INSERT INTO gold.DIM_PAR (\"PAR_SRK\", {colunas_sql})\n",
    "        VALUES (%s, {placeholders})\n",
    "        ON CONFLICT (\"PAR_SRK\") DO NOTHING;\n",
    "    \"\"\"\n",
    "\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",      \n",
    "        port=\"5432\",           \n",
    "        database=\"dados_inep\", \n",
    "        user=\"admin\",          \n",
    "        password=\"l1l2r1r2\" \n",
    "    )\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(sql_unknown, valores_unknown)\n",
    "        conn.commit()\n",
    "    conn.close()\n",
    "    print(\" Registro 'Unknown' (-1) verificado com sucesso.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Erro ao inserir registro Unknown: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
