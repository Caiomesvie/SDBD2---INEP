{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8062844",
   "metadata": {},
   "source": [
    "# ETL da camada bronze para camada silver\n",
    "\n",
    "Este notebook realiza o ETL dos dados da camada bronze para a camada silver. Ou seja: ele abre o dataset e o salva num dataframe, realiza a transformação dos dados e os carrega num arquivo `.csv` e no banco de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c77057",
   "metadata": {},
   "source": [
    "## EXTRACT\n",
    "# Estratégia de Leitura Escalável (PySpark)\n",
    "\n",
    "Para contornar as limitações de memória (especialmente em ambientes como WSL) ao ler o arquivo `MICRODADOS_ENEM_2021.csv` (>1.5GB), adotamos a estratégia de processamento distribuído com **Apache Spark**.\n",
    "\n",
    "Diferente do padrão do Pandas que tenta carregar todo o arquivo na RAM de uma vez, o PySpark utiliza o conceito de *Lazy Evaluation* (avaliação preguiçosa). Ele mapeia o arquivo e cria um plano de execução, mas só processa os dados na memória quando uma ação é solicitada, evitando o travamento do sistema.\n",
    "\n",
    "#### **Parâmetros Críticos:**\n",
    "\n",
    "* **`inferSchema=\"true\"`:** Permite que o Spark percorra os dados inicialmente para identificar automaticamente quais colunas são numéricas e quais são textuais, facilitando a análise imediata.\n",
    "* **`encoding=\"ISO-8859-1\"`:** Corrige erros de decodificação de caracteres que contem dentro do arquivo `MICRODADOS_ENEM_2021.csv` que e um \"erro\" bem comuns em dados  usado no Brasil.\n",
    "* **`delimiter=\";\"`:** Define o ponto e vírgula como o separador correto, evitando que o dataset seja interpretado equivocadamente como uma única coluna longa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326801bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RawToSilves\") \\\n",
    "    .config(\"spark.driver.memory\", \"5g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data_layer_filepath = '../raw/'\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \";\") \\\n",
    "    .option(\"encoding\", \"ISO-8859-1\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(data_layer_filepath + 'data_raw/MICRODADOS_ENEM_2021.csv')\n",
    "\n",
    "print(\"Arquivo completo mapeado com sucesso!\")\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421627a",
   "metadata": {},
   "source": [
    "## TRANSFORM\n",
    "### Padronização dos Nomes das Colunas.\n",
    "\n",
    "Inicialmente, o dataset contém colunas com nomes sem padrão, por \"sorte\" nessa basse que estamos usando todos os nome estão padronizado porem tudo com letras Maisculas. E para fins de tratamento iremos fazer uma varedura para que caso ajá alguma coluna que não eteja separam palavras com `_`, ou tenha alguma coluna com `whitespace`, passse para o padrão com as colunas seguindo esse novo padrão: **todos os caracteres em minúsculo, separando palavras com `_`**.\n",
    "\n",
    "\n",
    "Para renomear todas as colunas de uma vez convertendo para minúsculas e trocando espaços por `_`, a maneira mais eficiente quando se usa o **PySpark** é usar o método `.toDF().`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15443db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "novas_colunas = [col.lower().replace(' ', '_') for col in df.columns]\n",
    "\n",
    "df = df.toDF(*novas_colunas)\n",
    "\n",
    "print(df.columns)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7755801b",
   "metadata": {},
   "source": [
    "### Remoção de Colunas Desnecessárias\n",
    "\n",
    "Afim de melhorar como os dados vão ficar nessa etapa começamos a filtrar e retirar alguns dados são desnecesarios como por exemplo **nu_ano** já que toda a base e referente ao ano de 2021, e assim optamos por não trabalhar com essa coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5aa1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['nu_ano']\n",
    "\n",
    "df = df.drop(*cols_to_drop) \n",
    "\n",
    "for col in cols_to_drop:\n",
    "    if col not in df.columns:\n",
    "        print(f\"Coluna {col} deletada!\")\n",
    "\n",
    "print(\"Colunas restantes: \")\n",
    "print(df.columns)\n",
    "print(\" \")\n",
    "print(\"Tabelas restantes: \")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6997c8",
   "metadata": {},
   "source": [
    "### Correções dos Tipos de Dados.\n",
    "\n",
    "Nesta etapa, analisaremos a estrutura e a **tipagem dos dados** nas colunas para identificar inconsistências e realizar os tratamentos necessários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d4b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nome_arquivo = 'schema_enem.txt'\n",
    "# f = open(nome_arquivo, 'w')\n",
    "\n",
    "header = f\"{'COLUNA':<40} | {'TIPO'}\"\n",
    "separador = \"-\" * 50\n",
    "print(header)\n",
    "print(separador)\n",
    "\n",
    "# f.write(header + \"\\n\")\n",
    "# f.write(separador + \"\\n\")\n",
    "\n",
    "for coluna, tipo in df.dtypes:\n",
    "    linha = f\"{coluna:<40} | {tipo}\"\n",
    "    print(linha)\n",
    "    \n",
    "    # f.write(linha + \"\\n\")\n",
    "\n",
    "# f.close() \n",
    "# print(f\"\\nO arquivo '{nome_arquivo}' também foi salvo na pasta.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
